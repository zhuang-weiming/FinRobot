## 20250110
-------------------------------
| time/              |        |
|    fps             | 1558   |
|    iterations      | 450    |
|    time_elapsed    | 591    |
|    total_timesteps | 921600 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1558        |
|    iterations           | 451         |
|    time_elapsed         | 592         |
|    total_timesteps      | 923648      |
| train/                  |             |
|    approx_kl            | 0.029856816 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0811     |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.0226     |
|    std                  | 1.27        |
|    value_loss           | 0.000639    |
-----------------------------------------
Eval num_timesteps=925000, episode_reward=-1.64 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.64       |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.025110941 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0606     |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.0174     |
|    std                  | 1.28        |
|    value_loss           | 0.000325    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1557   |
|    iterations      | 452    |
|    time_elapsed    | 594    |
|    total_timesteps | 925696 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1558        |
|    iterations           | 453         |
|    time_elapsed         | 595         |
|    total_timesteps      | 927744      |
| train/                  |             |
|    approx_kl            | 0.028556168 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0909     |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.28        |
|    value_loss           | 0.000377    |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1558        |
|    iterations           | 454         |
|    time_elapsed         | 596         |
|    total_timesteps      | 929792      |
| train/                  |             |
|    approx_kl            | 0.035019614 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0464     |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0159     |
|    std                  | 1.28        |
|    value_loss           | 0.00151     |
-----------------------------------------
Eval num_timesteps=930000, episode_reward=-1.51 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.51       |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.029927386 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0554     |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.013      |
|    std                  | 1.28        |
|    value_loss           | 0.000233    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1558   |
|    iterations      | 455    |
|    time_elapsed    | 598    |
|    total_timesteps | 931840 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1558        |
|    iterations           | 456         |
|    time_elapsed         | 599         |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.022618432 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0543     |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.0143     |
|    std                  | 1.29        |
|    value_loss           | 0.000133    |
-----------------------------------------
Eval num_timesteps=935000, episode_reward=-1.57 +/- 0.00
Episode length: 143.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 143        |
|    mean_reward          | -1.57      |
| time/                   |            |
|    total_timesteps      | 935000     |
| train/                  |            |
|    approx_kl            | 0.03431194 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0588    |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.0205    |
|    std                  | 1.29       |
|    value_loss           | 0.00318    |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1557   |
|    iterations      | 457    |
|    time_elapsed    | 600    |
|    total_timesteps | 935936 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1558       |
|    iterations           | 458        |
|    time_elapsed         | 601        |
|    total_timesteps      | 937984     |
| train/                  |            |
|    approx_kl            | 0.02806526 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0528    |
|    n_updates            | 4570       |
|    policy_gradient_loss | -0.0232    |
|    std                  | 1.29       |
|    value_loss           | 0.00135    |
----------------------------------------
Eval num_timesteps=940000, episode_reward=-1.38 +/- 0.00
Episode length: 143.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 143        |
|    mean_reward          | -1.38      |
| time/                   |            |
|    total_timesteps      | 940000     |
| train/                  |            |
|    approx_kl            | 0.03056643 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0611    |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.0162    |
|    std                  | 1.3        |
|    value_loss           | 0.000975   |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1557   |
|    iterations      | 459    |
|    time_elapsed    | 603    |
|    total_timesteps | 940032 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1558       |
|    iterations           | 460        |
|    time_elapsed         | 604        |
|    total_timesteps      | 942080     |
| train/                  |            |
|    approx_kl            | 0.01975482 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0743    |
|    n_updates            | 4590       |
|    policy_gradient_loss | -0.0152    |
|    std                  | 1.3        |
|    value_loss           | 0.000201   |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1558        |
|    iterations           | 461         |
|    time_elapsed         | 605         |
|    total_timesteps      | 944128      |
| train/                  |             |
|    approx_kl            | 0.025191694 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0674     |
|    n_updates            | 4600        |
|    policy_gradient_loss | -0.0184     |
|    std                  | 1.3         |
|    value_loss           | 8.48e-05    |
-----------------------------------------
Eval num_timesteps=945000, episode_reward=-1.21 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.21       |
| time/                   |             |
|    total_timesteps      | 945000      |
| train/                  |             |
|    approx_kl            | 0.021826878 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0489     |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.0134     |
|    std                  | 1.3         |
|    value_loss           | 0.00392     |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1557   |
|    iterations      | 462    |
|    time_elapsed    | 607    |
|    total_timesteps | 946176 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1557        |
|    iterations           | 463         |
|    time_elapsed         | 608         |
|    total_timesteps      | 948224      |
| train/                  |             |
|    approx_kl            | 0.025606852 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.051      |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.0168     |
|    std                  | 1.31        |
|    value_loss           | 0.00139     |
-----------------------------------------
Eval num_timesteps=950000, episode_reward=-1.44 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.44       |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.032701578 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0473     |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.017      |
|    std                  | 1.31        |
|    value_loss           | 0.000623    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1557   |
|    iterations      | 464    |
|    time_elapsed    | 610    |
|    total_timesteps | 950272 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1557        |
|    iterations           | 465         |
|    time_elapsed         | 611         |
|    total_timesteps      | 952320      |
| train/                  |             |
|    approx_kl            | 0.023939611 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0583     |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0141     |
|    std                  | 1.31        |
|    value_loss           | 0.000655    |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1557        |
|    iterations           | 466         |
|    time_elapsed         | 612         |
|    total_timesteps      | 954368      |
| train/                  |             |
|    approx_kl            | 0.033731285 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0706     |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.0153     |
|    std                  | 1.32        |
|    value_loss           | 0.0012      |
-----------------------------------------
Eval num_timesteps=955000, episode_reward=-1.26 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.26       |
| time/                   |             |
|    total_timesteps      | 955000      |
| train/                  |             |
|    approx_kl            | 0.030104104 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0596     |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.0169     |
|    std                  | 1.32        |
|    value_loss           | 0.000602    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 467    |
|    time_elapsed    | 614    |
|    total_timesteps | 956416 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1557        |
|    iterations           | 468         |
|    time_elapsed         | 615         |
|    total_timesteps      | 958464      |
| train/                  |             |
|    approx_kl            | 0.050244257 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0624     |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.32        |
|    value_loss           | 0.000144    |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=-1.57 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.57       |
| time/                   |             |
|    total_timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.025381137 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.061      |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.32        |
|    value_loss           | 0.000103    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 469    |
|    time_elapsed    | 616    |
|    total_timesteps | 960512 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1557       |
|    iterations           | 470        |
|    time_elapsed         | 618        |
|    total_timesteps      | 962560     |
| train/                  |            |
|    approx_kl            | 0.03033777 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0662    |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.0127    |
|    std                  | 1.32       |
|    value_loss           | 4.99e-05   |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1557        |
|    iterations           | 471         |
|    time_elapsed         | 619         |
|    total_timesteps      | 964608      |
| train/                  |             |
|    approx_kl            | 0.029441144 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0682     |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.0209     |
|    std                  | 1.32        |
|    value_loss           | 0.000405    |
-----------------------------------------
Eval num_timesteps=965000, episode_reward=-1.43 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.43       |
| time/                   |             |
|    total_timesteps      | 965000      |
| train/                  |             |
|    approx_kl            | 0.036275826 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.057      |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.0165     |
|    std                  | 1.33        |
|    value_loss           | 0.000593    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 472    |
|    time_elapsed    | 620    |
|    total_timesteps | 966656 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1557        |
|    iterations           | 473         |
|    time_elapsed         | 622         |
|    total_timesteps      | 968704      |
| train/                  |             |
|    approx_kl            | 0.023099912 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0659     |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.0187     |
|    std                  | 1.32        |
|    value_loss           | 0.000806    |
-----------------------------------------
Eval num_timesteps=970000, episode_reward=-1.53 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.53       |
| time/                   |             |
|    total_timesteps      | 970000      |
| train/                  |             |
|    approx_kl            | 0.027546627 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0455     |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.0163     |
|    std                  | 1.32        |
|    value_loss           | 0.000775    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 474    |
|    time_elapsed    | 623    |
|    total_timesteps | 970752 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1556       |
|    iterations           | 475        |
|    time_elapsed         | 624        |
|    total_timesteps      | 972800     |
| train/                  |            |
|    approx_kl            | 0.03824235 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0521    |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.0129    |
|    std                  | 1.32       |
|    value_loss           | 0.000654   |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1557        |
|    iterations           | 476         |
|    time_elapsed         | 626         |
|    total_timesteps      | 974848      |
| train/                  |             |
|    approx_kl            | 0.028694471 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.055      |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 1.32        |
|    value_loss           | 0.000215    |
-----------------------------------------
Eval num_timesteps=975000, episode_reward=-1.16 +/- 0.00
Episode length: 143.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 143        |
|    mean_reward          | -1.16      |
| time/                   |            |
|    total_timesteps      | 975000     |
| train/                  |            |
|    approx_kl            | 0.02396845 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0734    |
|    n_updates            | 4760       |
|    policy_gradient_loss | -0.0187    |
|    std                  | 1.32       |
|    value_loss           | 0.000736   |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 477    |
|    time_elapsed    | 627    |
|    total_timesteps | 976896 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1556        |
|    iterations           | 478         |
|    time_elapsed         | 628         |
|    total_timesteps      | 978944      |
| train/                  |             |
|    approx_kl            | 0.030406108 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.068      |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.0224     |
|    std                  | 1.32        |
|    value_loss           | 0.000968    |
-----------------------------------------
Eval num_timesteps=980000, episode_reward=-1.98 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.98       |
| time/                   |             |
|    total_timesteps      | 980000      |
| train/                  |             |
|    approx_kl            | 0.022745036 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0528     |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.0172     |
|    std                  | 1.32        |
|    value_loss           | 0.00135     |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 479    |
|    time_elapsed    | 630    |
|    total_timesteps | 980992 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1556        |
|    iterations           | 480         |
|    time_elapsed         | 631         |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.027084397 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0741     |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.019      |
|    std                  | 1.32        |
|    value_loss           | 0.000577    |
-----------------------------------------
Eval num_timesteps=985000, episode_reward=-1.88 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.88       |
| time/                   |             |
|    total_timesteps      | 985000      |
| train/                  |             |
|    approx_kl            | 0.037456505 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0636     |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.017      |
|    std                  | 1.33        |
|    value_loss           | 0.000193    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 481    |
|    time_elapsed    | 632    |
|    total_timesteps | 985088 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1556        |
|    iterations           | 482         |
|    time_elapsed         | 634         |
|    total_timesteps      | 987136      |
| train/                  |             |
|    approx_kl            | 0.035046794 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0557     |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.0105     |
|    std                  | 1.33        |
|    value_loss           | 0.00311     |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1557       |
|    iterations           | 483        |
|    time_elapsed         | 635        |
|    total_timesteps      | 989184     |
| train/                  |            |
|    approx_kl            | 0.02870312 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0635    |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.0196    |
|    std                  | 1.34       |
|    value_loss           | 0.000748   |
----------------------------------------
Eval num_timesteps=990000, episode_reward=-1.73 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.73       |
| time/                   |             |
|    total_timesteps      | 990000      |
| train/                  |             |
|    approx_kl            | 0.026017418 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0689     |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.0178     |
|    std                  | 1.34        |
|    value_loss           | 0.000109    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 484    |
|    time_elapsed    | 636    |
|    total_timesteps | 991232 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1556        |
|    iterations           | 485         |
|    time_elapsed         | 638         |
|    total_timesteps      | 993280      |
| train/                  |             |
|    approx_kl            | 0.031352412 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0562     |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.0158     |
|    std                  | 1.34        |
|    value_loss           | 0.000171    |
-----------------------------------------
Eval num_timesteps=995000, episode_reward=-2.02 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -2.02       |
| time/                   |             |
|    total_timesteps      | 995000      |
| train/                  |             |
|    approx_kl            | 0.033567242 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.069      |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.35        |
|    value_loss           | 3.99e-05    |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1556   |
|    iterations      | 486    |
|    time_elapsed    | 639    |
|    total_timesteps | 995328 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1556       |
|    iterations           | 487        |
|    time_elapsed         | 640        |
|    total_timesteps      | 997376     |
| train/                  |            |
|    approx_kl            | 0.03007174 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0455    |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.0229    |
|    std                  | 1.35       |
|    value_loss           | 2.75e-05   |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1556        |
|    iterations           | 488         |
|    time_elapsed         | 642         |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.035235416 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0494     |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.0133     |
|    std                  | 1.35        |
|    value_loss           | 0.000421    |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=-1.96 +/- 0.00
Episode length: 143.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -1.96       |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.019933764 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0612     |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.0162     |
|    std                  | 1.34        |
|    value_loss           | 0.000759    |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 1556    |
|    iterations      | 489     |
|    time_elapsed    | 643     |
|    total_timesteps | 1001472 |
--------------------------------
 100% ━━━━━━━━━━━━━━━━ 1,001,472/1,00… [ 0:10:42 < 0:00:00 , 1,530 it/s ]

Mean evaluation reward: -1.97

从训练日志来看，有几个关键点需要分析：
训练稳定性指标:
explained_variance: 保持在 0.99 左右，表明值函数预测非常准确
approx_kl: 在 0.01-0.02 之间波动，表明策略更新稳定
clip_fraction: 在 0.1-0.16 之间，处于合理范围
entropy_loss: 从 -1.4 逐渐降至 -1.39，表明策略探索程度适中
1. 评估结果变化:
Eval num_timesteps=410000, episode_reward=-0.14
...
Eval num_timesteps=490000, episode_reward=0.00
模型从负收益逐渐改善到零收益，说明模型学会了规避风险。

2. 训练稳定性指标:
- explained_variance: 0.98-0.99，表明值函数预测非常准确
- approx_kl: 0.01-0.02，策略更新稳定
- clip_fraction: 0.11-0.16，处于合理范围
- entropy_loss: -1.41 到 -1.39，表明策略探索度适中
- value_loss: 从 0.025 降至 0.002-0.004，值函数学习良好

3. 存在的问题:
- 模型倾向于采取保守策略，最终收益趋近于0
- std (标准差) 维持在 0.97-0.99，表明动作选择的不确定性仍然较高

建议进一步改进：
1. 调整奖励权重 environment_trading.py:
```python
# 组合所有奖励
reward = (
    position_reward * 1.5 +    # 增加基础收益权重
    trend_reward * 0.8 +       # 增加趋势奖励权重
    rsi_reward * 0.5 +         # 增加RSI指标奖励权重
    position_size_reward * 0.3  # 增加持仓量奖励权重
) - transaction_cost * 0.8     # 降低交易成本惩罚
```

2. 优化网络结构 train_ppo.py:
```python
policy_kwargs=dict(
    net_arch=dict(
        pi=[512, 256, 256, 128],  # 进一步加深策略网络
        vf=[512, 256, 256, 128]   # 进一步加深价值网络
    ),
    activation_fn=nn.ReLU,
    # 添加层归一化
    normalize_images=False,
    features_extractor_kwargs=dict(
        features_dim=128
    )
)
```

3. 调整训练参数 train_ppo.py:
```python
model = PPO(
    "MlpPolicy", 
    env, 
    verbose=1,
    learning_rate=0.0002,      # 略微提高学习率
    n_steps=4096,              # 增加步数
    batch_size=512,            # 增加批量大小
    n_epochs=15,               # 增加每批次训练轮数
    gamma=0.995,               # 提高折扣因子
    gae_lambda=0.98,           # 提高GAE参数
    clip_range=0.3,            # 增加裁剪范围
    ent_coef=0.03,            # 进一步增加熵系数
    max_grad_norm=0.7,         # 放宽梯度裁剪
    tensorboard_log="./ppo_stock_tensorboard/"
)
```

4. 增加训练步数和评估频率:
```python
total_timesteps = 2000000  # 增加到200万步

eval_callback = EvalCallback(
    eval_env,
    best_model_save_path='./best_model/',
    log_path='./logs/',
    eval_freq=2500,          # 更频繁的评估
    n_eval_episodes=10,      # 增加评估轮数
    deterministic=True,
    render=False
)
```


## 20250106
run log:
2025-01-06 10:12:41,327 - __main__ - INFO - 开始训练Agent...
2025-01-06 10:12:42,449 - src.agent - INFO - Episode 1, Reward: [-0.65130121]
2025-01-06 10:12:42,983 - src.agent - INFO - Episode 2, Reward: [-0.65097582]
2025-01-06 10:12:43,472 - src.agent - INFO - Episode 3, Reward: [-0.65224018]
2025-01-06 10:12:43,956 - src.agent - INFO - Episode 4, Reward: [-0.64703998]
2025-01-06 10:12:44,454 - src.agent - INFO - Episode 5, Reward: [-0.65284646]
2025-01-06 10:12:44,986 - src.agent - INFO - Episode 6, Reward: [-0.65729351]
2025-01-06 10:12:45,466 - src.agent - INFO - Episode 7, Reward: [-0.65158149]
2025-01-06 10:12:45,970 - src.agent - INFO - Episode 8, Reward: [-0.65702845]
2025-01-06 10:12:46,490 - src.agent - INFO - Episode 9, Reward: [-0.64290715]
2025-01-06 10:12:46,960 - src.agent - INFO - Episode 10, Reward: [-0.65098072]
2025-01-06 10:12:46,960 - src.agent - INFO - Agent训练完成

### 问题分析

1. 训练奖励偏低
观察到每个 Episode 的奖励都为负数且数值较低，这通常意味着智能体在环境中持续亏损或获得的奖励信号非常稀疏和负面。可能的原因包括：
- 奖励函数设计不合理： 您当前的奖励函数可能过于严苛，使得即使是合理的交易行为也难以获得正向奖励。例如，如果奖励仅仅基于最终的盈亏，而没有考虑中间过程的风险调整或夏普比率等因素，可能会导致奖励信号不稳定。
- 探索不足或探索策略不当： 如果智能体过度依赖已知的策略，而没有充分探索更有潜力的行动空间，可能会陷入局部最优解，导致奖励停滞在较低水平。
- 超参数设置不当： 学习率、折扣因子 gamma、探索噪声的参数等设置可能不适合当前的环境和任务，导致学习过程不稳定或收敛缓慢。
- 状态表示不充分： 智能体接收到的状态信息可能不足以让其做出明智的决策。例如，可能缺少关键的市场信息或技术指标。
- 网络结构或参数不合适： Actor 和 Critic 网络的结构可能过于简单或复杂，或者初始化参数不当，影响学习效果。
- 环境的随机性或复杂性： 股票市场的 inherent 随机性和复杂性可能使得智能体在短时间内难以学习到有效的策略。
- 训练步数不足： 从日志来看，训练只进行了 10 个 Episodes，这对于复杂的强化学习任务来说可能远远不够。

2. 预测价格严重失真
预测的光大证券股价在 0.1～0.9 元之间，与实际价格 17.00 元相差甚远，这表明预测模型存在严重的问题。可能的原因包括：
- 数据标准化/归一化问题： 如果训练数据被标准化或归一化到特定范围（例如 0 到 1 或 -1 到 1），而预测结果没有进行反标准化/反归一化，就会导致预测值与实际值的尺度不一致。
- 训练数据与预测数据不一致： 训练模型使用的数据特征和尺度可能与用于预测的数据不一致。
- 模型未充分训练或欠拟合： 如果模型没有经过充分的训练，可能无法捕捉到股票价格的真实波动模式。
- 模型容量不足： 模型结构可能过于简单，无法学习到复杂的非线性关系。
- 特征工程不足： 提供的输入特征可能不足以准确预测股价。
- 时间序列预测的特殊性： 股票价格是时间序列数据，具有自相关性和趋势性。如果模型没有考虑到这些特性（例如使用循环神经网络），可能难以做出准确预测。
- 环境模拟与真实市场差异过大： 您构建的交易环境可能与真实股票市场的运行机制存在较大差异，导致模型在模拟环境中学习到的策略在真实市场中失效。
- 价格限制处理不当： 在环境和模型中对涨跌停的处理方式可能导致预测值被限制在一个不合理的范围内。
- Bug 存在： 代码中可能存在影响数据处理、模型训练或预测的错误。

### 修改方案
针对以上分析，我将提供一些修改建议，您可以逐步尝试：

#### 针对训练奖励偏低：
```python
# environment.py
class AStockTradingEnv(gym.Env):
    # ... existing code ...
    def step(self, action):
        # ... existing code ...
        reward = self._calculate_reward(action) # 使用新的奖励计算方法
        # ... existing code ...

    def _calculate_reward(self, action):
        """重新设计的奖励函数"""
        current_value = self.amount + np.sum(self.stocks * self.data['close'])
        reward = current_value - self.last_value  # 基于账户价值的变化
        self.last_value = current_value
        return reward * self.reward_scaling
```
解释：
- 在 AStockTradingEnv 和 SingleStockTradingEnv 中，添加或修改 _calculate_reward 方法。
- 将奖励函数修改为基于账户价值的变化，这能更直接地反映交易行为的效果。
- 保留 reward_scaling 以便调整奖励的幅度。

```python
# agent.py
class DDPGAgent:
    def __init__(self, env, gamma=0.99, tau=0.005, lr_actor=1e-4, lr_critic=1e-3, exploration_noise_std=0.1): # 添加探索噪声参数
        # ... existing code ...
        self.exploration_noise_std = exploration_noise_std

    def act(self, state, deterministic=False):
        state = torch.FloatTensor(state).unsqueeze(0)
        action = self.actor(state).detach().numpy()[0]
        if not deterministic:
            action += np.random.normal(0, self.exploration_noise_std, size=self.env.action_space.shape) # 使用可调节的噪声
        return np.clip(action, -1, 1)

    def update(self):
        # ... existing code ...
        self.noise_std = max(self.min_noise, self.noise_std * self.noise_decay) # 如果您使用了噪声衰减

```
解释：
- 在 DDPGAgent 的 __init__ 方法中添加 exploration_noise_std 参数，用于控制探索的强度。
- 在 act 方法中使用该参数来调整探索噪声的大小。
- 您可以根据需要调整 exploration_noise_std 的初始值，并在训练过程中可能进行衰减。
```python
# stock_prediction.py
# ... existing code ...
# 示例参数
learning_rate = 1e-3  # 适当提高学习率
batch_size = 128      # 适当增加批量大小
buffer_size = 100000  # 适当增加经验回放缓冲区大小
total_timesteps = 100000 # 增加训练总步数/Episodes
# ... existing code ...
```

解释：
- 适当调整学习率、批量大小、经验回放缓冲区大小和训练总步数，这些超参数对强化学习算法的性能至关重要。您可以尝试不同的值，并观察训练效果。

#### 针对预测价格严重失真：
```python
# preprocessor.py
class DataPreprocessor:
    def __init__(self):
        self.scalers = {}

    def preprocess_data(self, raw_df, tech_indicator_list=None):
        # ... existing code ...
        processed_df = processed_df.dropna() # 在计算技术指标后删除 NaN 值
        return processed_df

    def standardize(self, df):
        """标准化数据，保存标准化参数用于反标准化"""
        for col in df.columns:
            if col not in ['date', 'tic']:
                self.scalers[col] = {
                    'mean': df[col].mean(),
                    'std': df[col].std()
                }
                df[col] = (df[col] - self.scalers[col]['mean']) / self.scalers[col]['std']
        return df

    def inverse_standardize(self, df):
        """反标准化数据"""
        for col in df.columns:
            if col in self.scalers:
                df[col] = df[col] * self.scalers[col]['std'] + self.scalers[col]['mean']
        return df
```
解释：
- 确保在计算技术指标后删除可能产生的 NaN 值，避免影响后续的标准化和模型训练。
- 仔细检查 standardize 和 inverse_standardize 方法的实现，确保标准化和反标准化的逻辑正确。

```python
# agent.py
class Actor(nn.Module):
    # ... existing code ...

class DDPGAgent:
    # ... existing code ...

def create_and_train_agent(env_train, batch_size, buffer_size, learning_rate, net_arch, total_timesteps):
    # ... existing code ...
    agent = DDPGAgent(env_train, lr_actor=learning_rate, lr_critic=learning_rate) # 确保学习率传递给 Agent
    # ... existing code ...

def predict_with_agent(agent, environment, deterministic=True):
    # ... existing code ...
    df_account_value['price'] = environment.preprocessor.inverse_standardize(df_account_value[['price']]).values
    # ... existing code ...

```
解释：
- 确保在创建 DDPGAgent 实例时，将 create_and_train_agent 函数中传入的 learning_rate 参数正确传递给 Actor 和 Critic 的优化器。
- 在 predict_with_agent 函数中，对预测出的价格进行反标准化操作，使用在训练数据上计算得到的标准化参数。这里假设您的 environment 对象可以访问 preprocessor。

```python
# single_stock_env.py
class SingleStockTradingEnv(gym.Env):
    # ... existing code ...
    def reset(self, *, seed = None, options = None):
        super().reset(seed=seed)
        self.day = 0
        self.amount = self.initial_amount
        self.stocks = 0
        self.last_action = 0 # 初始化 last_action
        self.last_value = self.initial_amount
        self.done = False
        self.history = []
        self.current_state = self._get_obs()
        return self.current_state, {}

    def step(self, action):
        # ... existing code ...
        # 缩放动作到实际交易单位，例如：
        real_action = int(np.round(action * self.hmax))
        # ... 使用 real_action 进行交易 ...
        self.last_action = real_action # 更新 last_action
        # ... existing code ...

```

解释：
- 在 SingleStockTradingEnv 的 reset 方法中初始化 last_action 为 0。
- 在 step 方法中，将模型输出的动作值（通常在 -1 到 1 之间）缩放到实际的交易单位，例如股票数量。self.hmax 代表最大交易单位。
- 更新 self.last_action 的值为实际执行的交易动作。

```python
# stock_prediction.py
# ... existing code ...
tech_indicator_list = [
    'macd', 'rsi', 'cci', 'boll_ub', 'boll_lb', 'atr', 'kst',
    'trix', 'roc',  # 常用技术指标
]
# ... existing code ...

```
解释：
- 在 stock_prediction.py 中，考虑添加更多相关的技术指标到 tech_indicator_list 中，以提供更丰富的状态信息。

### 其他建议：
- 增加训练数据量： 尝试使用更长时间跨度的数据进行训练。
- 调整网络结构： 可以尝试增加 Actor 和 Critic 网络的层数或神经元数量，或者尝试使用更复杂的网络结构。
- 使用更先进的算法： 如果 DDPG 的效果不理想，可以考虑尝试其他更适合时间序列预测的强化学习算法，例如 TD3 或 SAC。
- 更精细的特征工程： 除了常用的技术指标外，可以考虑加入成交量、市场情绪等其他可能影响股价的因素。
- 仔细检查数据处理流程： 确保从数据加载、预处理到环境状态构建的每个环节都没有错误。
- 逐步调试： 一次只修改一个地方，并仔细观察修改后的效果，以便定位问题。
- 可视化更多信息： 除了账户价值，还可以可视化股票持有量、交易行为等，以帮助理解智能体的行为。